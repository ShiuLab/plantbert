env:
  rand_seed : 20231002
  model_dir_name : 'models'
  data_dir_name : 'data'
  test_size : 0.1

tokenize:
  vocab_size : 30_522
  max_length : 512

pretrain:
  mlm : True    # train with masked language modeling
  # probability of masking each token
  mlm_prob : 0.2
  # evaluate each `logging_steps` steps
  eval_by : 'steps'
  overwrite_out : True
  # num of training epochs, feel free to tweak
  num_epochs : 10
  # per device training batch size, depend on GPU memory
  train_batch_size : 20
  # accumulate gradients before weights update 
  grad_acc_steps : 8
  # per device evaluation batch size
  eval_batch_size : 64
  # evaluate, log and save every 500 steps
  log_steps : 500
  save_steps : 500
  # save SafeTensors instead of Tensors 
  safetensors : True
  load_best : True
  save_limit : 3